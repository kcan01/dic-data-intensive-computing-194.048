runners:
  hadoop:
    jobconf:
      # Resource allocation
      mapreduce.map.memory.mb: 4096
      mapreduce.reduce.memory.mb: 8192
      mapreduce.map.java.opts: -Xmx3072m
      mapreduce.reduce.java.opts: -Xmx6144m

      # Parallelism settings
      mapreduce.job.reduces: 8
      mapreduce.job.maps: 16

      # Input split size (smaller splits = more mappers)
      mapreduce.input.fileinputformat.split.minsize: 67108864  # 64MB
      mapreduce.input.fileinputformat.split.maxsize: 134217728  # 128MB

      # Ensure no combiner usage if that's causing issues
      mapreduce.map.combine.minspills: 0  # Only use combiner if spilled at least this many times

      # Performance optimization
      mapreduce.task.io.sort.mb: 800
      mapreduce.task.io.sort.factor: 64
      mapreduce.map.sort.spill.percent: 0.90

      # I/O optimization
      mapreduce.map.output.compress: true
      mapreduce.map.output.compress.codec: org.apache.hadoop.io.compress.SnappyCodec
      mapreduce.output.fileoutputformat.compress: false

      # Speculative execution
      mapreduce.map.speculative: true
      mapreduce.reduce.speculative: true

      # Split size configuration
      mapreduce.input.fileinputformat.split.maxsize: 134217728


    # mrjob specific settings
    max_map_tasks: 16
    max_reduce_tasks: 8
    task_map_memory: 4096M
    task_reduce_memory: 8192M
    compression_codec: null