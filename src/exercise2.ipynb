{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DiC Assignment 2\n",
    "\n",
    "**Group 6:**\n",
    " Theresa Mayer\n",
    " Theresa Bruckner\n",
    " Jan Tölken\n",
    " Can Kenan Kandil \n",
    " Thomas Klar\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6001bd1cd5bbeecc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Spark Session Creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87585586debee5d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, ChiSqSelector, IndexToString, StringIndexer, CountVectorizer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Assignment2\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:03.522454900Z",
     "start_time": "2025-04-30T08:32:53.098387600Z"
    }
   },
   "id": "71a0152cc7b39c",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a0df0004ee49a7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:03.524455100Z",
     "start_time": "2025-04-30T08:33:03.521437800Z"
    }
   },
   "id": "a7eaeda76730d351",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2\n",
    "\n",
    "We want to convert the review texts to a classic vector space representation with TFIDF-weighted features based on the\n",
    "Spark DataFrame/Dataset API by building a transformation pipeline. The primary goal of this part is the\n",
    "preparation of the pipeline for Part 3.\n",
    "\n",
    "We start by loading the data into a Spark DataFrame."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb7e8356017720c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path = \"reviews_devset.json\" #\"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "input_file = spark.read.format(\"json\").load(path).select(\"category\", \"reviewText\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:09.123790300Z",
     "start_time": "2025-04-30T08:33:03.527471800Z"
    }
   },
   "id": "e7b1f20b244ab0a3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "input_file.show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:09.501495Z",
     "start_time": "2025-04-30T08:33:09.124782600Z"
    }
   },
   "id": "e3b86ac2dbe88c0e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label Encoding\n",
    "\n",
    "As the first step, we perform Label Encoding to convert the category strings into integers. To retransform them if nescessary, we also create a Reindexing Transformer. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "386773c21dd2c42f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "indexModel = indexer.fit(input_file)\n",
    "input_file_1 = indexModel.transform(input_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:11.950750800Z",
     "start_time": "2025-04-30T08:33:09.505501200Z"
    }
   },
   "id": "f74a275212c51ff1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|            category|          reviewText|label|  category_reindexed|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|For the most part...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...| 18.0|Patio_Lawn_and_Garde|\n",
      "+--------------------+--------------------+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "reindexer = IndexToString(inputCol=indexer.getOutputCol(), outputCol=\"category_reindexed\")\n",
    "reindexer.transform(input_file_1).show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:12.354857100Z",
     "start_time": "2025-04-30T08:33:11.957748600Z"
    }
   },
   "id": "91dbb482ceee28c8",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization\n",
    "As the next step, we tokenize the reviews into words. To do this, we split at whitespaces, tables, digits, and all the symbols given in the regex pattern below. Additionally, this tokenizer also performs Case folding and can filter out tokens with only one character."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3de1257a194d0d98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='reviewText', \n",
    "                           outputCol='tokens', \n",
    "                           pattern=r\"[ \\t\\d(){}\\[\\].!?;:,\\-=\\\"~#@&*%€$§\\\\'\\n\\r\\/]+\", \n",
    "                           minTokenLength=2, \n",
    "                           toLowercase=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:12.375524300Z",
     "start_time": "2025-04-30T08:33:12.348844200Z"
    }
   },
   "id": "86f44a30b94fde24",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|            category|          reviewText|label|              tokens|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...| 18.0|[this, was, gift,...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...| 18.0|[this, is, very, ...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...| 18.0|[the, metal, base...|\n",
      "|Patio_Lawn_and_Garde|For the most part...| 18.0|[for, the, most, ...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...| 18.0|[this, hose, is, ...|\n",
      "+--------------------+--------------------+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "input_2 = tokenizer.transform(input_file_1)\n",
    "input_2.show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:12.821861100Z",
     "start_time": "2025-04-30T08:33:12.365518900Z"
    }
   },
   "id": "73744aba0411c769",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stopword Removal\n",
    "The next transformer can filter out the stopwords in the tokens. We use the same stopswords as in the previous exercise. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2809b821557c1231"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "stopword_file = \"stopwords.txt\"\n",
    "with open(stopword_file, 'r', encoding='utf-8') as f:\n",
    "    # Strip whitespace and convert to lowercase\n",
    "    stopwords = [line.strip() for line in f]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:12.871428400Z",
     "start_time": "2025-04-30T08:33:12.805342400Z"
    }
   },
   "id": "8a7de5f2412c7f14",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                                    outputCol=\"tokens_nostop\",\n",
    "                                    stopWords=stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:13.061743400Z",
     "start_time": "2025-04-30T08:33:12.833432800Z"
    }
   },
   "id": "eff2476bc98647e2",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              tokens|       tokens_nostop|\n",
      "+--------------------+--------------------+\n",
      "|[this, was, gift,...|[gift, husband, m...|\n",
      "|[this, is, very, ...|[nice, spreader, ...|\n",
      "|[the, metal, base...|[metal, base, hos...|\n",
      "|[for, the, most, ...|[part, works, pre...|\n",
      "|[this, hose, is, ...|[hose, supposed, ...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "input_3 = stopword_remover.transform(input_2)\n",
    "input_3.select(\"tokens\", \"tokens_nostop\").show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:13.601739700Z",
     "start_time": "2025-04-30T08:33:13.008732900Z"
    }
   },
   "id": "1ae56d94bfefafe0",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Calculation\n",
    "The Calculation of the TF-IDF Vectors from the list of tokens is performed in two steps. The first uses _CountVectorizer_ to calculate Term Frequencies, outputting Sparse Vectors. We use a fixed Vocabulary Size of 60 000 tokens, which might be unnecessary large. The _CountVectorizer_ also allows us to access the vocabulary at the end to extract the most important words from it. The second step uses _IDF_ to scale the term frequency vectors with the inverse document frequencies. \n",
    "\n",
    "It should be noted that those are both Estimators, not Transformers, and thus require fitting. In the Pipeline, those Estimators are fitted on the training data when calling _pipeline.fit()_ and then also transform the training data. The test data is only transformed when calling _pipeline.transform()_, since the Estimators are already fitted. This also ensure that there is no Data Leakage from the test data. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4bd4517a28d0224"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf = CountVectorizer(inputCol=stopword_remover.getOutputCol(), \n",
    "                      outputCol=\"tf_output\", \n",
    "                      vocabSize=60_000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:13.663777300Z",
     "start_time": "2025-04-30T08:33:13.606743500Z"
    }
   },
   "id": "520f4384afb1c02a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=tf.getOutputCol(), \n",
    "          outputCol=\"tfidf_output\",\n",
    "          minDocFreq=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:13.704767400Z",
     "start_time": "2025-04-30T08:33:13.643751600Z"
    }
   },
   "id": "18f6a5940ffa4f7a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       tokens_nostop|           tf_output|\n",
      "+--------------------+--------------------+\n",
      "|[gift, husband, m...|(60000,[2,3,7,8,3...|\n",
      "|[nice, spreader, ...|(60000,[0,1,3,21,...|\n",
      "|[metal, base, hos...|(60000,[4,10,29,1...|\n",
      "|[part, works, pre...|(60000,[1,3,4,9,1...|\n",
      "|[hose, supposed, ...|(60000,[12,32,42,...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "tfmodel = tf.fit(input_3)\n",
    "input_4 = tfmodel.transform(input_3)\n",
    "input_4.select(\"tokens_nostop\", \"tf_output\").show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:18.007209100Z",
     "start_time": "2025-04-30T08:33:13.662778300Z"
    }
   },
   "id": "c16333ea627b73ef",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           tf_output|        tfidf_output|\n",
      "+--------------------+--------------------+\n",
      "|(60000,[2,3,7,8,3...|(60000,[2,3,7,8,3...|\n",
      "|(60000,[0,1,3,21,...|(60000,[0,1,3,21,...|\n",
      "|(60000,[4,10,29,1...|(60000,[4,10,29,1...|\n",
      "|(60000,[1,3,4,9,1...|(60000,[1,3,4,9,1...|\n",
      "|(60000,[12,32,42,...|(60000,[12,32,42,...|\n",
      "+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "idfModel = idf.fit(input_4)\n",
    "input_5 = idfModel.transform(input_4)\n",
    "input_5.select(\"tf_output\", \"tfidf_output\").show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:21.424457200Z",
     "start_time": "2025-04-30T08:33:17.997207500Z"
    }
   },
   "id": "671b0e229579f64",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Selection of top 2000 features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "687262ca63c769e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chisq = ChiSqSelector(featuresCol=idf.getOutputCol(),\n",
    "                      labelCol=\"label\",\n",
    "                      outputCol=\"features\",\n",
    "                      numTopFeatures=2000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:21.474367200Z",
     "start_time": "2025-04-30T08:33:21.429469400Z"
    }
   },
   "id": "5838bd7b13080160",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(2000,[2,3,7,8,35...|\n",
      "|(2000,[0,1,3,21,3...|\n",
      "|(2000,[4,10,174,3...|\n",
      "|(2000,[1,3,4,9,10...|\n",
      "|(2000,[12,29,101,...|\n",
      "+--------------------+\n"
     ]
    }
   ],
   "source": [
    "chisqModel = chisq.fit(input_5)\n",
    "input_6 = chisqModel.transform(input_5)\n",
    "input_6.select(\"features\").show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:28.952445700Z",
     "start_time": "2025-04-30T08:33:21.449368200Z"
    }
   },
   "id": "e886fe30ccd73050",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline Creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "515f5142fe2926a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_pipeline(n_features=2000):\n",
    "    chisq.setNumTopFeatures(n_features)\n",
    "    pipeline = Pipeline(stages=[\n",
    "        indexer,\n",
    "        tokenizer,\n",
    "        stopword_remover,\n",
    "        tf,\n",
    "        idf,\n",
    "        chisq\n",
    "    ])\n",
    "    return pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:28.954444800Z",
     "start_time": "2025-04-30T08:33:28.948444100Z"
    }
   },
   "id": "da44c70940813a8b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 18.0|(2000,[2,3,7,8,35...|\n",
      "| 18.0|(2000,[0,1,3,21,3...|\n",
      "| 18.0|(2000,[4,10,174,3...|\n",
      "| 18.0|(2000,[1,3,4,9,10...|\n",
      "| 18.0|(2000,[12,29,101,...|\n",
      "+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_pipeline(n_features=2000)\n",
    "preprocessing_pipeline = pipeline.fit(input_file)\n",
    "preprocessing_pipeline.transform(input_file).select(\"label\", \"features\").show(n=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:38.004774300Z",
     "start_time": "2025-04-30T08:33:28.955444600Z"
    }
   },
   "id": "7e782a230d004d18",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export most important tokens to file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c248123a0fdd46a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_top_terms_from_pipeline(pipeline):\n",
    "    n = len(pipeline.stages[5].selectedFeatures)\n",
    "\n",
    "\n",
    "    vocab = pipeline.stages[3].vocabulary.copy()\n",
    "    top_words = \" \".join(sorted([vocab[i] for i in pipeline.stages[5].selectedFeatures]))\n",
    "    \n",
    "    with open(\"output_ds.txt\", \"w\") as f:\n",
    "        f.write(top_words)\n",
    "        \n",
    "    return n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:38.016781100Z",
     "start_time": "2025-04-30T08:33:38.003268500Z"
    }
   },
   "id": "2db6ff9b908acf67",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_terms_from_pipeline(preprocessing_pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:38.254068500Z",
     "start_time": "2025-04-30T08:33:38.009790800Z"
    }
   },
   "id": "a16d8f0f587ffeb3",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aad6541e9575ab6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To develop the SVM you can use this dataframe for testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d11efb0d9d96c1d0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 18.0|(2000,[2,3,7,8,35...|\n",
      "| 18.0|(2000,[0,1,3,21,3...|\n",
      "| 18.0|(2000,[4,10,174,3...|\n",
      "| 18.0|(2000,[1,3,4,9,10...|\n",
      "| 18.0|(2000,[12,29,101,...|\n",
      "| 18.0|(2000,[0,3,4,8,11...|\n",
      "| 18.0|(2000,[18,112,175...|\n",
      "| 18.0|(2000,[6,21,32,36...|\n",
      "| 18.0|(2000,[3,4,5,6,40...|\n",
      "| 18.0|(2000,[6,8,38,78,...|\n",
      "| 18.0|(2000,[1,13,226],...|\n",
      "| 18.0|(2000,[5,17,33,40...|\n",
      "| 18.0|(2000,[1,11,28,35...|\n",
      "| 18.0|(2000,[40,144,339...|\n",
      "| 18.0|(2000,[0,3,7,9,11...|\n",
      "| 18.0|(2000,[8,26,57,80...|\n",
      "| 18.0|(2000,[1,15,120,1...|\n",
      "| 18.0|(2000,[2,3,221,26...|\n",
      "| 18.0|(2000,[4,10,16,20...|\n",
      "| 18.0|(2000,[0,18,30,42...|\n",
      "+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "input_test = preprocessing_pipeline.transform(input_file).select(\"label\", \"features\")\n",
    "input_test.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:38.706055200Z",
     "start_time": "2025-04-30T08:33:38.251084400Z"
    }
   },
   "id": "b4fb3c4371331825",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are finished and have a working SVM, you can create an end to end pipeline like this: "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d28a32c3ceb330"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_model_pipeline(n_features=2000, add_more_arguments_here=None):\n",
    "    chisq.setNumTopFeatures(n_features)\n",
    "    # set model parameters here or something probably\n",
    "    \n",
    "    \n",
    "    model_pipeline = Pipeline(stages=[\n",
    "        indexer,\n",
    "        tokenizer,\n",
    "        stopword_remover,\n",
    "        tf,\n",
    "        idf,\n",
    "        chisq,\n",
    "        # add all new transformers/estimators here \n",
    "    ])\n",
    "    return model_pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-30T08:33:38.715357100Z",
     "start_time": "2025-04-30T08:33:38.709369900Z"
    }
   },
   "id": "5718d45a74de417f",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then you can do end-to-end testing with train and test set similar to this:\n",
    "\n",
    "testset, trainset = split(input_data)\n",
    "model_pipeline = get_model_pipeline(parameters)\n",
    "model = model_pipeline.fit(trainset)\n",
    "output = model.transform(testset)\n",
    "calculate_metrics(output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4261ec537d72535"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
