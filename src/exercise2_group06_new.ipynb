{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f0d5fe3cd758e9",
   "metadata": {},
   "source": [
    "#### DiC Assignment 2\n",
    "\n",
    "Group 6\n",
    "Members:\n",
    " Theresa Mayer\n",
    " Theresa Bruckner\n",
    " Jan Tölken\n",
    " Can Kenan Kandil \n",
    " Thomas Klar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789a489f-c347-43e2-a596-e2193fc19681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1329a003-f1a7-4457-8ccc-4cca290e5566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, IDF, ChiSqSelector, IndexToString, StringIndexer, CountVectorizer, Normalizer\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196340eb-c112-450e-a2fb-73e865e0caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkFiles\n",
    "import json\n",
    "import re\n",
    "from heapq import nlargest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611c83d4-d18f-4e26-a448-965f0f1d81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviews_devset.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba032eb1-61ac-4f6d-b87e-d862ac92d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4060. Attempting port 4061.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4061. Attempting port 4062.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4062. Attempting port 4063.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4063. Attempting port 4064.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4064. Attempting port 4065.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4065. Attempting port 4066.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4066. Attempting port 4067.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4067. Attempting port 4068.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4068. Attempting port 4069.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4069. Attempting port 4070.\n",
      "25/05/13 11:37:37 WARN Utils: Service 'SparkUI' could not bind on port 4070. Attempting port 4071.\n",
      "25/05/13 11:37:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Assignment2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae20b2d40936f49",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6286c105-854c-4036-b715-6fa72f8ef670",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_json = \"categories.json\"\n",
    "stopwords_txt   = \"stopwords.txt\"\n",
    "output_path     = \"chi_square_rdd.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f11805-ad79-46ab-b3b6-8fbac49254c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f79a18bd-c520-4998-b5a2-3d8c9fcb1670",
   "metadata": {},
   "outputs": [],
   "source": "docs = sc.textFile(path)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94980e19-a314-4017-9b70-ba05bbb687b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df   = spark.read.json(path).select(\"category\",\"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72151b2-9c58-4d4a-a2dc-3c85c2e1300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.rdd.map(lambda row: json.dumps({\n",
    "    \"category\":   row.category,\n",
    "    \"reviewText\": row.reviewText\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45c6b27-3568-44ae-88dd-91a6e4a5a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addFile(categories_json)\n",
    "sc.addFile(stopwords_txt)\n",
    "\n",
    "categories_map = json.load(open(SparkFiles.get(categories_json), encoding='utf-8'))\n",
    "stopwords      = set(line.strip() for line in open(SparkFiles.get(stopwords_txt), encoding='utf-8'))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regex similar to Assignment 1\n",
   "id": "fe8ccc45ca765e4a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f803a4-6f94-4fc2-94c8-963a9e5e4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_on = r\"[ \\t\\d(){}\\[\\].!?;:,+=\\\"~#@&*%€$§\\\\'\\n\\r/-]+\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9e07a78-38d0-4b91-b37b-cf2938bb26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = re.compile(split_on)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### implementing broadcasts for efficiency\n",
   "id": "b4ef23d533263406"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e925b53e-dd8e-4433-bdd1-6f7444375b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_categories = sc.broadcast(categories_map)\n",
    "bc_stopwords  = sc.broadcast(stopwords)\n",
    "bc_splitter   = sc.broadcast(splitter)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper functions",
   "id": "954988abba77edd6"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a32b2c-8363-4aec-a61b-93b763c06231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(line, categories_map, stopwords, splitter):\n",
    "    \"\"\"\n",
    "    - function for determining the category, putting the text in lower case, performing the tokenization and stopword removal\n",
    "    The functionality is imitated according to mapper_count from Assignment 1\n",
    "    \"\"\"\n",
    "    doc = json.loads(line)\n",
    "    cat = categories_map.get(doc.get('category',''), 'X')\n",
    "    text = doc.get('reviewText','').lower()\n",
    "    tokens = set(splitter.split(text))\n",
    "    tokens = tokens.difference(stopwords)\n",
    "    return [((token, cat), 1) for token in tokens if len(token) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b518d12-fe6c-412a-a991-7e027797df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chi(item):\n",
    "    \"\"\"\n",
    "    calculating the chi-square values similar to the reducer_calc_chisq\n",
    "    \"\"\"\n",
    "    term, counts = item\n",
    "    counts_dict = dict(counts)\n",
    "    n_term = sum(counts_dict.values())\n",
    "    N      = total_docs\n",
    "    out    = []\n",
    "    for cat, n_cat in bc_cat_counts.value.items():\n",
    "        A = counts_dict.get(cat, 0)\n",
    "        B = n_term - A\n",
    "        C = n_cat   - A\n",
    "        D = N - n_cat - B\n",
    "        denom = (A + B)*(A + C)*(B + D)*(C + D)\n",
    "        if denom > 0:\n",
    "            chi = float(N)*(A*D - B*C)**2/denom\n",
    "            out.append((cat, (term, chi)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130f1b13-745c-42d6-8379-f507e8c884b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "docs = sc.textFile(path)\n",
    "total_docs = docs.count()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### implementing category counts - similar to the CategoryCounter job of MapReduce\n",
   "id": "804a2d1d0ca48091"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8596879f-2958-4031-8a19-1d15ea51803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "cat_counts = (\n",
    "    docs\n",
    "    .map(lambda line: json.loads(line).get('category',''))\n",
    "    .map(lambda c: (bc_categories.value.get(c,'X'), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .collectAsMap()\n",
    ")\n",
    "bc_cat_counts = sc.broadcast(cat_counts)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### implementing term-category-counts - similar to mapper_count (by using the function extract terms)\n",
   "id": "b244f7ee8ad085c1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dde9a0e-fdcb-4a95-ad26-0823c0822e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_cat_counts = (\n",
    "    docs\n",
    "    .flatMap(lambda line: extract_terms(\n",
    "        line,\n",
    "        bc_categories.value,\n",
    "        bc_stopwords.value,\n",
    "        bc_splitter.value\n",
    "    ))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### grouping by term\n",
   "id": "47963560980ed4a9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f89c2d04-68af-4fd9-b99d-5a455eb1af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_counts = (\n",
    "    term_cat_counts\n",
    "    .map(lambda kv: (kv[0][0], (kv[0][1], kv[1])))\n",
    "    .groupByKey()\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### calculating the chi values",
   "id": "2d09165ed4347b92"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9c2264-0a77-4793-813a-585016126201",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_terms = term_counts.flatMap(compute_chi)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Top75",
   "id": "89e0e2f6f96885cc"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a440a03-21c3-49be-9296-34aa60afa13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "top_terms = (\n",
    "    chi_terms\n",
    "    .groupByKey()\n",
    "    .mapValues(lambda vals: nlargest(75, vals, key=lambda x: x[1]))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Formatting the output as in Assignment1\n",
    " "
   ],
   "id": "eaa3917ec13f6b5c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c77b1189-308c-4ff4-9339-87c9b8f7c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for cat, terms in sorted(top_terms, key=lambda x: x[0]):\n",
    "        cat_name = next((k for k, v in categories_map.items() if v == cat), cat)\n",
    "        elems = [f\"{t}:{chi:.2f}\" for t, chi in terms]\n",
    "        f.write(cat_name + \" \" + \" \".join(elems) + \"\\n\")\n",
    "    merged = sorted({t for _, terms in top_terms for t, _ in terms})\n",
    "    f.write(\" \".join(merged))\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a9e367e35c11e",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bdeceef-1edf-4080-a505-f0e298ec4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4056. Attempting port 4057.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4057. Attempting port 4058.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4058. Attempting port 4059.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4059. Attempting port 4060.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4060. Attempting port 4061.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4061. Attempting port 4062.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4062. Attempting port 4063.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4063. Attempting port 4064.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4064. Attempting port 4065.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4065. Attempting port 4066.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4066. Attempting port 4067.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4067. Attempting port 4068.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4068. Attempting port 4069.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4069. Attempting port 4070.\n",
      "25/05/13 11:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4070. Attempting port 4071.\n",
      "25/05/13 11:40:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Assignment2\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfccc3fb3838c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "input_file = spark.read.format(\"json\").load(path).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "186764fd21395d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|          reviewText|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_file.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca71a16b6ebab12",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf48e2c8ff605c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "indexModel = indexer.fit(input_file)\n",
    "input_file_1 = indexModel.transform(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3a4842704c53eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|            category|          reviewText|label|  category_reindexed|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|For the most part...| 18.0|Patio_Lawn_and_Garde|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...| 18.0|Patio_Lawn_and_Garde|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reindexer = IndexToString(inputCol=indexer.getOutputCol(), outputCol=\"category_reindexed\")\n",
    "reindexer.transform(input_file_1).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc1ba2834236ae",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "480053bd32182782",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='reviewText', outputCol='tokens', pattern=r\"[ \\t\\d(){}\\[\\].!?;:,\\-=\\\"~#@&*%€$§\\\\'\\n\\r\\/]+\", minTokenLength=2, toLowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ee4084fb6dba2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|            category|          reviewText|label|              tokens|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...| 18.0|[this, was, gift,...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...| 18.0|[this, is, very, ...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...| 18.0|[the, metal, base...|\n",
      "|Patio_Lawn_and_Garde|For the most part...| 18.0|[for, the, most, ...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...| 18.0|[this, hose, is, ...|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_2 = tokenizer.transform(input_file_1)\n",
    "input_2.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8b1644a45b0f5",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b738a865a9f6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = \"stopwords.txt\"\n",
    "with open(stopword_file, 'r', encoding='utf-8') as f:\n",
    "    # Strip whitespace and convert to lowercase\n",
    "    stopwords = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7eb7e272386741bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                                    outputCol=\"tokens_nostop\",\n",
    "                                    stopWords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d25eac03a1a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              tokens|       tokens_nostop|\n",
      "+--------------------+--------------------+\n",
      "|[this, was, gift,...|[gift, husband, m...|\n",
      "|[this, is, very, ...|[nice, spreader, ...|\n",
      "|[the, metal, base...|[metal, base, hos...|\n",
      "|[for, the, most, ...|[part, works, pre...|\n",
      "|[this, hose, is, ...|[hose, supposed, ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_3 = stopword_remover.transform(input_2)\n",
    "input_3.select(\"tokens\", \"tokens_nostop\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd46ce49f28ba6",
   "metadata": {},
   "source": [
    "### TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d4d8da1188230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = CountVectorizer(inputCol=stopword_remover.getOutputCol(), \n",
    "                      outputCol=\"tf_output\", \n",
    "                      vocabSize=40_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6457adb373059d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=tf.getOutputCol(), \n",
    "          outputCol=\"tfidf_output\",\n",
    "          minDocFreq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1677eda57f22e7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       tokens_nostop|           tf_output|\n",
      "+--------------------+--------------------+\n",
      "|[gift, husband, m...|(40000,[2,3,7,8,3...|\n",
      "|[nice, spreader, ...|(40000,[0,1,3,21,...|\n",
      "|[metal, base, hos...|(40000,[4,10,29,1...|\n",
      "|[part, works, pre...|(40000,[1,3,4,9,1...|\n",
      "|[hose, supposed, ...|(40000,[12,32,42,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfmodel = tf.fit(input_3)\n",
    "input_4 = tfmodel.transform(input_3)\n",
    "input_4.select(\"tokens_nostop\", \"tf_output\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecc723e4d8e30864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:42:10 WARN DAGScheduler: Broadcasting large task binary with size 1082.1 KiB\n",
      "+--------------------+--------------------+\n",
      "|           tf_output|        tfidf_output|\n",
      "+--------------------+--------------------+\n",
      "|(40000,[2,3,7,8,3...|(40000,[2,3,7,8,3...|\n",
      "|(40000,[0,1,3,21,...|(40000,[0,1,3,21,...|\n",
      "|(40000,[4,10,29,1...|(40000,[4,10,29,1...|\n",
      "|(40000,[1,3,4,9,1...|(40000,[1,3,4,9,1...|\n",
      "|(40000,[12,32,42,...|(40000,[12,32,42,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfModel = idf.fit(input_4)\n",
    "input_5 = idfModel.transform(input_4)\n",
    "input_5.select(\"tf_output\", \"tfidf_output\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2936347dfba753",
   "metadata": {},
   "source": [
    "### Selection of top 2000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b75fa2122a0a5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = ChiSqSelector(featuresCol=idf.getOutputCol(),\n",
    "                      labelCol=\"label\",\n",
    "                      outputCol=\"features\",\n",
    "                      numTopFeatures=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7be11c9695ec26fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:42:11 WARN DAGScheduler: Broadcasting large task binary with size 1093.4 KiB\n",
      "25/05/13 11:42:11 WARN DAGScheduler: Broadcasting large task binary with size 1095.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:42:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:42:44 WARN DAGScheduler: Broadcasting large task binary with size 1087.3 KiB\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(2000,[2,3,7,8,35...|\n",
      "|(2000,[0,1,3,21,3...|\n",
      "|(2000,[4,10,174,3...|\n",
      "|(2000,[1,3,4,9,10...|\n",
      "|(2000,[12,29,101,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chisqModel = chisq.fit(input_5)\n",
    "input_6 = chisqModel.transform(input_5)\n",
    "input_6.select(\"features\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661448bb93880ad",
   "metadata": {},
   "source": [
    "### Pipeline Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "859f91ddad18d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(n_features=2000):\n",
    "    chisq.setNumTopFeatures(n_features)\n",
    "    pipeline = Pipeline(stages=[\n",
    "        indexer,\n",
    "        tokenizer,\n",
    "        stopword_remover,\n",
    "        tf,\n",
    "        idf,\n",
    "        chisq\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f8f79f353b60726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:43:12 WARN DAGScheduler: Broadcasting large task binary with size 1093.4 KiB\n",
      "25/05/13 11:43:12 WARN DAGScheduler: Broadcasting large task binary with size 1095.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:43:25 WARN DAGScheduler: Broadcasting large task binary with size 1097.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/13 11:43:44 WARN DAGScheduler: Broadcasting large task binary with size 1093.3 KiB\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 18.0|(2000,[2,3,7,8,35...|\n",
      "| 18.0|(2000,[0,1,3,21,3...|\n",
      "| 18.0|(2000,[4,10,174,3...|\n",
      "| 18.0|(2000,[1,3,4,9,10...|\n",
      "| 18.0|(2000,[12,29,101,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = get_pipeline(n_features=2000)\n",
    "preprocessing_pipeline = pipeline.fit(input_file)\n",
    "preprocessing_pipeline.transform(input_file).select(\"label\", \"features\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592ae48a18656be",
   "metadata": {},
   "source": [
    "### Export most important tokens to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693cff8a0fb7a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms_from_pipeline(pipeline):\n",
    "    n = len(pipeline.stages[5].selectedFeatures)\n",
    "\n",
    "\n",
    "    vocab = pipeline.stages[3].vocabulary.copy()\n",
    "    top_words = \" \".join(sorted([vocab[i] for i in pipeline.stages[5].selectedFeatures]))\n",
    "    \n",
    "    with open(\"output_ds.txt\", \"w\") as f:\n",
    "        f.write(top_words)\n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2e411ee7c5aa577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_terms_from_pipeline(preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e8d22c8cf6a73",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41c2dfe88437e5",
   "metadata": {},
   "source": [
    "For this part we create a svm classifier to predict the categories based on their review text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0dfb8648f458d",
   "metadata": {},
   "source": [
    "First we import the necessary libraries from pyspark,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb9bd7e41d2ca431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "#to ignore the warning messages coming from the parameter grid search when training tvs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d30c62eb6cf2b",
   "metadata": {},
   "source": [
    "Now we add the needed steps for our pipeline, starting with a normalizer with L2 norm by using p=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58b496931e27f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizer using features from the ChiSquare step and output should be the normalized features\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norm_features\", p =2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee9adb4b9c05db",
   "metadata": {},
   "source": [
    "Next we create the classifier and add One-vs-Rest since SVM is only binary and we have a multi-class-problem. For further explanation on why OVR is used see the report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ebff49435c37304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM uses the normalized features and the created label from the indexer\n",
    "svm = LinearSVC(labelCol=\"label\", featuresCol=\"norm_features\")\n",
    "ovr = OneVsRest(classifier=svm, labelCol=\"label\", featuresCol=\"norm_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e36f14e70cf69b",
   "metadata": {},
   "source": [
    "Now we create the get_full_pipeline function to be able to switch between used features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42fe8cdc5a082124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline with additional normalization and SVM with OVR \n",
    "def get_full_pipeline(n_features=2000):\n",
    "    chisq.setNumTopFeatures(n_features)\n",
    "    full_pipeline = Pipeline(stages=[\n",
    "    indexer,\n",
    "    tokenizer,\n",
    "    stopword_remover,\n",
    "    tf,\n",
    "    idf,\n",
    "    chisq,\n",
    "    normalizer,\n",
    "    ovr\n",
    "    ])\n",
    "    return full_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d440379e0dacb3",
   "metadata": {},
   "source": [
    "Now we create the parameter grid to compare the input parameters for SVM to get the best performing model based on F1-Score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f46f5f1e4cac0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0])\n",
    "    .addGrid(svm.maxIter, [10, 100])\n",
    "    .addGrid(svm.standardization, [True, False])\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a91f952bcbc4",
   "metadata": {},
   "source": [
    "Now we split the input data into training and test (we also had to desample the input file, since otherwise the training of the classifier in combination with the parameter grid would take too long). We decided to use 30% of the input data, since with this split we get a good ratio between computing time and sample size. \n",
    "\n",
    "Here we dont split the data into training and validation since this will be done automaticall in the function TrainValidationSplit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d0a78e90c77862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset \n",
    "devset_sample, devset_rest = input_file.randomSplit([0.3,0.7], seed = 1234)\n",
    "train_data, test_data = devset_sample.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad0466a9756b7348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used data for modeling has 23665 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has 18905 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " so 3781.0 rows will be used for validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(f\"used data for modeling has {devset_sample.count()} rows\")\n",
    "print(f\"training data has {train_data.count()} rows\")\n",
    "print(f\" so {train_data.count() * 0.2} rows will be used for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cd3734dc01b29",
   "metadata": {},
   "source": [
    "Lastly we need an evaluator with F1-metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4af69d37dfbbbfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcf454ebb97779",
   "metadata": {},
   "source": [
    "## Model with 2000 features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a27d6247d49646",
   "metadata": {},
   "source": [
    "Now we train the model using the top 2000 features. We start with getting the full pipeline with n_features = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d570ff411c0a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full pipeline with 2000 features\n",
    "full_pipeline = get_full_pipeline(n_features=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c990e4f17e8c1b",
   "metadata": {},
   "source": [
    "Now we set up the TrainValidationSplit setup for the computation later on. Here we predefined that we use 80% of the data for training and the remaining 20% for validation for parameter decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac0300f5052de59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit Setup\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=full_pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "298bf43e6e3dc9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at 2025-05-13 11:43:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 191:>  (0 + 0) / 2][Stage 192:=> (1 + 1) / 2][Stage 194:>  (0 + 1) / 2]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:853\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m start_time_readable \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mfromtimestamp(start_time)\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mH:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mM:\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mS\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstarting at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_time_readable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \n\u001B[0;32m----> 5\u001B[0m tvs_model \u001B[38;5;241m=\u001B[39m \u001B[43mtvs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfit_time=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfit_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m/usr/lib/spark/python/pyspark/ml/tuning.py:1464\u001B[0m, in \u001B[0;36mTrainValidationSplit._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m   1462\u001B[0m pool \u001B[38;5;241m=\u001B[39m ThreadPool(processes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetParallelism(), numModels))\n\u001B[1;32m   1463\u001B[0m metrics \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m numModels\n\u001B[0;32m-> 1464\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[1;32m   1465\u001B[0m     metrics[j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[1;32m   1466\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[0;32m/usr/lib64/python3.9/multiprocessing/pool.py:858\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 858\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    860\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m/usr/lib64/python3.9/threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Training with 2000 features \n",
    "start_time = time.time()\n",
    "start_time_readable = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"starting at {start_time_readable}\") \n",
    "tvs_model = tvs.fit(train_data)\n",
    "fit_time = time.time() - start_time\n",
    "print(f'fit_time={fit_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3513430ff7cbaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b96e575c20506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc21e63c1eb6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce224b1c6fd4046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ad88e07cf21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_time=3726.6424901485443 bei 20 parallelism\n",
    "#fit_time=2063.811951637268 bei 50 parallelism \n",
    "#fit_time=2226.436852455139 bei 100 \n",
    "#fit_time=3263.985917568207 bei 50 parallelism und 30% von devset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed15f2ea945a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_predictions = tvs_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc97f203ea429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tvs_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cc5beab2acb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_model = best_model.stages[-1] \n",
    "best_svm_model = ovr_model.getClassifier()\n",
    "\n",
    "# Show Parameters\n",
    "print(\"Best model:\")\n",
    "print(f\"  regParam:        {best_svm_model.getRegParam()}\")\n",
    "print(f\"  maxIter:         {best_svm_model.getMaxIter()}\")\n",
    "print(f\"  standardization: {best_svm_model.getStandardization()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dc643f559f9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1 = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Test f1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8680f7641abe63f9",
   "metadata": {},
   "source": [
    "Validation f1: 0.5712\n",
    "Test f1: 0.5680"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe45f5614ef156c",
   "metadata": {},
   "source": [
    "## Model with 500 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dea54a46195f9",
   "metadata": {},
   "source": [
    "Now we do the same with only 500 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b2221717617fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_500 = get_full_pipeline(n_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849780aebc819244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit Setup\n",
    "tvs_500 = TrainValidationSplit(\n",
    "    estimator=full_pipeline_500,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,\n",
    "    parallelism=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96085ba459a565f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with 500 features \n",
    "start_time = time.time()\n",
    "start_time_readable = datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"starting at {start_time_readable}\") \n",
    "tvs_model_500 = tvs_500.fit(train_data)\n",
    "fit_time = time.time() - start_time\n",
    "print(f'fit_time={fit_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6113c835bf2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "test_predictions_500 = tvs_model_500.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143d92598fd0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_500 = tvs_model_500.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24597d55cbe60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_model_500 = best_model_500.stages[-1] \n",
    "best_svm_model_500 = ovr_model_500.getClassifier()\n",
    "\n",
    "# Show best parameters\n",
    "print(\"Best model:\")\n",
    "print(f\"  regParam:        {best_svm_model_500.getRegParam()}\")\n",
    "print(f\"  maxIter:         {best_svm_model_500.getMaxIter()}\")\n",
    "print(f\"  standardization: {best_svm_model_500.getStandardization()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658ccd742af76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f1_500 = evaluator.evaluate(test_predictions_500)\n",
    "\n",
    "print(f\"Test f1: {test_f1_500:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e55b1b10bcc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df481336d3dc0e1",
   "metadata": {},
   "source": [
    "## Final turn with full model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaabf5870ed9312",
   "metadata": {},
   "source": [
    "After getting the best parameter, we will create the final svm with these parameters and create a new pipeline for the full dataset (so no parameter optimization has to be done on the full set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc05218b7f32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs:///user/dic25_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "final_file = spark.read.format(\"json\").load(path).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9681a876e69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_file.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9eadd7f534738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regParam = best_svm_model.getRegParam()\n",
    "best_maxIter = best_svm_model.getMaxIter() \n",
    "best_standardization = best_svm_model.getStandardization()\n",
    "best_n_features  = 2000\n",
    "\n",
    "svm_final = LinearSVC(labelCol=\"label\", featuresCol=\"norm_features\", regParam = best_regParam, maxIter = best_maxIter, standardization = best_standardization)\n",
    "ovr_final = OneVsRest(classifier=svm_final, labelCol=\"label\", featuresCol=\"norm_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51967699b31474da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_pipeline(n_features=best_n_features):\n",
    "    chisq.setNumTopFeatures(n_features)\n",
    "    full_pipeline = Pipeline(stages=[\n",
    "    indexer,\n",
    "    tokenizer,\n",
    "    stopword_remover,\n",
    "    tf,\n",
    "    idf,\n",
    "    chisq,\n",
    "    normalizer,\n",
    "    ovr\n",
    "    ])\n",
    "    return full_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01030a0ad1ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Pipeline for full set \n",
    "final_pipeline = get_final_pipeline(n_features=best_n_features)\n",
    "\n",
    "final_model = final_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f2991c142dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.transform(test_data)\n",
    "final_f1 = evaluator.evaluate(final_predictions)\n",
    "\n",
    "print(f\"Test f1: {final_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf5c27d2b178cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3e363424c13001",
   "metadata": {},
   "source": [
    "### Additional part only for Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748c1eb09466bf9",
   "metadata": {},
   "source": [
    "This part is only used for comparison of different parameters, in finished code this should be commented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e5ae8ddb7c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for protocoll to see how the different settings perform \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Loop über alle Parameterkombinationen\n",
    "for params in paramGrid:\n",
    "    # Setze die Parameter im SVM-Objekt\n",
    "    for param, value in params.items():\n",
    "        svm._set(**{param.name: value})\n",
    "\n",
    "    # Pipeline neu aufbauen mit aktuellem SVM\n",
    "    current_pipeline = full_pipeline.copy()\n",
    "    current_pipeline.setStages(full_pipeline.getStages()[:-1] + [OneVsRest(classifier=svm)])\n",
    "\n",
    "    # Trainiere das Modell\n",
    "    model = current_pipeline.fit(train_data)\n",
    "\n",
    "    # Vorhersage auf Validierungsdaten\n",
    "    predictions = model.transform(val_data)\n",
    "\n",
    "    # F1-Score berechnen\n",
    "    f1 = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Parameter & Score speichern\n",
    "    results.append((params, f1))\n",
    "\n",
    "    # Ausgabe\n",
    "    print(\"Param-Kombi:\")\n",
    "    for p, v in params.items():\n",
    "        print(f\"  {p.name}: {v}\")\n",
    "    print(f\"  → F1-Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ee33a895285ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 194:=> (1 + 1) / 2][Stage 196:>  (0 + 1) / 2][Stage 198:>  (0 + 0) / 2]  "
     ]
    }
   ],
   "source": [
    "best_params, best_f1 = max(results, key=lambda x: x[1])\n",
    "print(f\"Best combination → F1: {best_f1:.4f}\")\n",
    "for p, v in best_params.items():\n",
    "    print(f\"{p.name}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8f265-3878-413f-8af9-b950fc179e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bdee0a-dfe7-4197-9a8d-4aef82eaec59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808171f8-4c39-4690-94fe-cfa213a43292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a4ad7-b694-4127-83b2-f5210b49da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0e3fc-c461-4cac-bca9-8752e662d743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae902518-3380-4ac0-8336-2468d0f468d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57add27c-ada9-4aea-83f6-839422a4d7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
